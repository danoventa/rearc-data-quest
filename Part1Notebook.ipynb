{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import urllib\n",
    "import boto3\n",
    "import os\n",
    "import re\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "from botocore.client import Config\n",
    "from urllib.request import urlopen\n",
    "\n",
    "\n",
    "class FileSyncer:\n",
    "    def __init__(self, host_url: str, s3_bucket: str) -> None:\n",
    "        # Set up class variables for extract\n",
    "        self.host_url = host_url\n",
    "\n",
    "        # set up class variables for load\n",
    "        self.prefix = \"productivity_cost\"\n",
    "        self.files_pending_upload = []\n",
    "\n",
    "        self.files_to_add = set()\n",
    "        self.files_up_to_date = set()\n",
    "        self.files_to_delete = set()\n",
    "        \n",
    "        s3 = boto3.resource('s3')\n",
    "        self.bucket = s3.Bucket( \"noventa-scratch-bucket\")\n",
    "        self.s3_bucket = s3_bucket\n",
    "        self.s3_client = boto3.client('s3', 'us-east-2', config=Config(signature_version='s3v4'))\n",
    "        self.ses_client = boto3.client('ses', 'us-east-2')\n",
    "        \n",
    "        # Configure URLLib to mimic a browser\n",
    "        opener = urllib.request.build_opener()\n",
    "        opener.addheaders = [(\"User-Agent\", \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36\")]\n",
    "        urllib.request.install_opener(opener)\n",
    "\n",
    "        # Create tmp directory if it doesn't exist\n",
    "        if not os.path.exists(\"./tmp\"):\n",
    "            os.makedirs(\"./tmp\")\n",
    "\n",
    "\n",
    "    # Private helper functions for Syncer\n",
    "    def _clean_up_local_file(self, filename: str) -> None:\n",
    "        os.remove(f\"./tmp/{filename}\")\n",
    "\n",
    "    def _check_files_to_update(self) -> None:\n",
    "        logging.log(logging.INFO, f\"Checking files to be updated...\")\n",
    "\n",
    "        s3_files = set()\n",
    "        for obj in self.bucket.objects.filter(Prefix=\"productivity_cost/\"):\n",
    "            _, existing_file_date, existing_file = obj.key.split('/')\n",
    "            s3_files.add((existing_file_date, existing_file))\n",
    "\n",
    "        files_pending_upload = set(self.files_pending_upload)\n",
    "        self.files_to_add = files_pending_upload - s3_files\n",
    "        self.files_up_to_date = files_pending_upload & s3_files\n",
    "        self.files_to_delete = s3_files - files_pending_upload\n",
    "\n",
    "        # If an updated file is in files_to_add, remove it from files_up_to_date and add it to files_to_delete\n",
    "        for _, add_file in self.files_to_add:\n",
    "            for update_date, update_file in self.files_up_to_date.copy():\n",
    "                if add_file == update_file:\n",
    "                    self.files_up_to_date.remove((update_date, update_file))\n",
    "                    self.files_to_delete.add((update_date, update_file))\n",
    "\n",
    "        logging.log(logging.INFO, f\"Staging the following files to upload: {self.files_to_add}\")\n",
    "        logging.log(logging.INFO, f\"Staging the following files to delete: {self.files_to_delete}\")\n",
    "        logging.log(logging.INFO, f\"The following files are up to date: {self.files_up_to_date}\")\n",
    "\n",
    "\n",
    "    def _create_date_directory(self, date: str) -> None:\n",
    "        # Create tmp directory if it doesn't exist\n",
    "        if not os.path.exists(f\"./tmp/{date}\"):\n",
    "            logging.log(logging.INFO, f\"Creating directory for date: {date}\")\n",
    "            os.makedirs(f\"./tmp/{date}\")\n",
    "\n",
    "    # Public functions for Syncer\n",
    "    def generate_presigned_urls(self, expires_in: int=604800) -> list[str]:\n",
    "        available_files = self.files_up_to_date.union(self.files_to_add)\n",
    "\n",
    "        presigned_urls = []\n",
    "        for date, file in available_files:\n",
    "            key = f\"{self.prefix}/{date}/{file}\"\n",
    "            logging.log(logging.INFO, f\"Generating presigned URL for key: {key}\")\n",
    "            presigned_urls.append((date, file, self.s3_client.generate_presigned_url('get_object',\n",
    "                                            Params={'Bucket': self.s3_bucket,\n",
    "                                                    'Key': f\"{key}\"},\n",
    "                                            ExpiresIn=expires_in)))\n",
    "\n",
    "        return presigned_urls            \n",
    "\n",
    "    def send_email(self, email: str = \"noventa@hey.com\") -> None:\n",
    "        html_list = '<ol>'\n",
    "        for date, file, item in self.generate_presigned_urls():\n",
    "            html_list += f'<li>{date}/{file}: {item}</li>'\n",
    "        html_list += '</ol>'          \n",
    "\n",
    "        logging.log(logging.INFO, f\"Sending email to {email}\")\n",
    "        self.ses_client.send_email(\n",
    "            Destination={\n",
    "                'ToAddresses': [email],\n",
    "            },\n",
    "            Message={\n",
    "                'Body': {\n",
    "                    'Html': {\n",
    "                        'Charset': 'UTF-8',\n",
    "                        'Data': f'<h1>Presigned Urls</h1>{html_list}',\n",
    "                    },\n",
    "                },\n",
    "                'Subject': {\n",
    "                    'Charset': 'UTF-8',\n",
    "                    'Data': 'Sending Presigned URLS',\n",
    "                },\n",
    "            },\n",
    "            Source=email,\n",
    "        )\n",
    "\n",
    "    def extract_productivity_cost_data(self) -> None:\n",
    "        # get latest file list\n",
    "        logging.log(logging.INFO, f\"Extracting productivity cost data from {self.host_url}\")\n",
    "        with urlopen(f\"{self.host_url}/pub/time.series/pr/\") as response:\n",
    "            body = response.read()\n",
    "            soup = BeautifulSoup(body, 'html.parser')\n",
    "\n",
    "            dates = []\n",
    "            for date in re.findall(\"[0-9]{1,2}/[0-9]{1,2}/[0-9]{4}\", str(soup.find_all('pre')[0])):\n",
    "                dates.append(datetime.strptime(date.strip(), '%m/%d/%Y').strftime('%Y-%m-%d'))\n",
    "\n",
    "            SKIP_FIRST_INDEX = slice(1, None, None)\n",
    "            file_index = 0\n",
    "            for link in soup.find_all('a')[SKIP_FIRST_INDEX]:        \n",
    "                file_date = dates[file_index]\n",
    "                file_index += 1\n",
    "\n",
    "                file_name = link.get('href').split(\"/\")[-1]\n",
    "\n",
    "                self._create_date_directory(file_date)\n",
    "\n",
    "                urllib.request.urlretrieve (f\"{self.host_url}{link.get('href')}\", f\"./tmp/{file_date}/{file_name}\")\n",
    "                logging.log(logging.INFO, f\"Downloaded file: {file_date}/{file_name}\")\n",
    "                self.files_pending_upload.append((file_date, file_name))\n",
    "\n",
    "\n",
    "    def load_productivity_cost_data(self) -> None:\n",
    "        self._check_files_to_update()\n",
    "\n",
    "        logging.log(logging.INFO, f\"Adding the following new files: {self.files_to_add}\")\n",
    "        for file_date, file_name in self.files_to_add:\n",
    "            object_key = f\"{self.prefix}/{file_date}/{file_name}\"\n",
    "            self.bucket.upload_file(f'./tmp/{file_date}/{file_name}', f\"{object_key}\")\n",
    "\n",
    "        logging.log(logging.INFO, f\"Deleting the following files: {self.files_to_delete}\")\n",
    "        for file_date, file_name in self.files_to_delete:\n",
    "            object_key = f\"{self.prefix}/{file_date}/{file_name}\"\n",
    "            self.s3_client.delete_objects(f's3://{self.s3_bucket}/{object_key}')\n",
    "\n",
    "    def clean_up_local_files(self) -> None:\n",
    "        for file_date, file_name in self.files_pending_upload:\n",
    "            logging.log(logging.INFO, f\"Cleaning up local file: {file_date}/{file_name}\")\n",
    "            self._clean_up_local_file(f\"{file_date}/{file_name}\")\n",
    "\n",
    "    def purge_local_directory(self) -> None:\n",
    "        for date in os.listdir(\"./tmp\"):\n",
    "            logging.log(logging.INFO, f\"Removing local directory: {date}\")\n",
    "            os.rmdir(f\"./tmp/{date}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "host_url = \"https://download.bls.gov\"\n",
    "sync = FileSyncer(host_url, \"noventa-scratch-bucket\")\n",
    "\n",
    "sync.extract_productivity_cost_data()\n",
    "sync.load_productivity_cost_data()\n",
    "sync.send_email()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sync.clean_up_local_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "sync.purge_local_directory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MessageId': '010f0190086f5896-42b305fd-8e70-4169-b004-3be32f9192ae-000000', 'ResponseMetadata': {'RequestId': 'b49029f9-4e3f-4d1a-8e7b-44f6302444b2', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Tue, 11 Jun 2024 17:53:53 GMT', 'content-type': 'text/xml', 'content-length': '326', 'connection': 'keep-alive', 'x-amzn-requestid': 'b49029f9-4e3f-4d1a-8e7b-44f6302444b2'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
